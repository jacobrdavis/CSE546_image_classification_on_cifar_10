{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jacobrdavis/CSE546_image_classification_on_cifar_10/blob/main/CSE546_image_classification_on_cifar_10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UNmkdWoXvkBE"
      },
      "source": [
        "## Image Classification on CIFAR-10\n",
        "In this problem we will explore different deep learning architectures for image classification on the CIFAR-10 dataset. Make sure that you are familiar with torch `Tensor`s, two-dimensional convolutions (`nn.Conv2d`) and fully-connected layers (`nn.Linear`), ReLU non-linearities (`F.relu`), pooling (`nn.MaxPool2d`), and tensor reshaping (`view`).\n",
        "\n",
        "We will use Colab because it has free GPU runtimes available; GPUs can accelerate training times for this problem by 10-100x. **You will need to enable the GPU runtime to use it**. To do so, click \"Runtime\" above and then \"Change runtime type\". There under hardware accelerator choose \"GPU\".\n",
        "\n",
        "This notebook provides some starter code for the CIFAR-10 problem on HW4, including a completed training loop to assist with some of the Pytorch setup. You'll need to modify this code to implement the layers required for the assignment, but this provides a working training loop to start from.\n",
        "\n",
        "*Note: GPU runtimes are limited on Colab. Limit your training to short-running jobs (around 20mins or less) and spread training out over time, if possible. Colab WILL limit your usage of GPU time, so plan ahead and be prepared to take breaks during training.* We also suggest performing your early coding/sweeps on a small fraction of the dataset (~10%) to minimize training time and GPU usage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bb7WymOmv_cx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.distributions import uniform\n",
        "import numpy as np\n",
        "\n",
        "from typing import Tuple, Union, List, Callable\n",
        "from torch.optim import SGD\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-SusLoH91CEz"
      },
      "source": [
        "Let's verify that we are using a gpu:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmxNdxwvxNs1",
        "outputId": "74958020-ad21-44f4-957f-50e2ee395718"
      },
      "outputs": [],
      "source": [
        "# assert torch.cuda.is_available(), \"GPU is not available, check the directions above (or disable this assertion to use CPU)\"\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(DEVICE)  # this should print out CUDA"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "LbFk2t2RxYcn"
      },
      "source": [
        "To use the GPU you will need to send both the model and data to a device; this transfers the model from its default location on CPU to the GPU.\n",
        "\n",
        "Note that torch operations on Tensors will fail if they are not located on the same device.\n",
        "\n",
        "```python\n",
        "model = model.to(DEVICE)  # Sending a model to GPU\n",
        "\n",
        "for x, y in tqdm(data_loader):\n",
        "  x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "```\n",
        "When reading tensors you may need to send them back to cpu, you can do so with `x = x.cpu()`."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xODE5P5D1Wwy"
      },
      "source": [
        "Let's load CIFAR-10 data. This is how we load datasets using PyTorch in the real world!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOvLuZry1cKc",
        "outputId": "919741fd-dace-4745-e7a5-251ff10a9f08"
      },
      "outputs": [],
      "source": [
        "train_dataset = torchvision.datasets.CIFAR10(\"./data\", train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
        "test_dataset = torchvision.datasets.CIFAR10(\"./data\", train=False, download=True, transform=torchvision.transforms.ToTensor())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "oG78KMOj61HJ"
      },
      "source": [
        "Here, we'll use the torch `DataLoader` to wrap our datasets. `DataLoader`s handle batching, shuffling, and iterating over data; they can also be useful for building more complex input pipelines that perform transfoermations such as data augmentation."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ju8QRAyv2u7F"
      },
      "source": [
        "## For Reference: Logistic Regression\n",
        "\n",
        "This problem is about deep learning architectures, not pytorch. We are providing an implementation of logistic regression using SGD in torch, which can serve as a template for the rest of your implementation in this problem."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ELjydYRt5Frf"
      },
      "source": [
        "Before we get started, let's take a look at our data to get an understanding of what we are doing. CIFAR-10 is a dataset containing images split into 10 classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "id": "_UU8aIle5m8Q",
        "outputId": "11a4b439-04f1-42e7-a16e-a50f2561f25b"
      },
      "outputs": [],
      "source": [
        "# imgs, labels = next(iter(train_loader))\n",
        "# print(f\"A single batch of images has shape: {imgs.size()}\")\n",
        "# example_image, example_label = imgs[0], labels[0]\n",
        "# c, w, h = example_image.size()\n",
        "# print(f\"A single RGB image has {c} channels, width {w}, and height {h}.\")\n",
        "\n",
        "# # This is one way to flatten our images\n",
        "# batch_flat_view = imgs.view(-1, c * w * h)\n",
        "# print(f\"Size of a batch of images flattened with view: {batch_flat_view.size()}\")\n",
        "\n",
        "# # This is another equivalent way\n",
        "# batch_flat_flatten = imgs.flatten(1)\n",
        "# print(f\"Size of a batch of images flattened with flatten: {batch_flat_flatten.size()}\")\n",
        "\n",
        "# # The new dimension is just the product of the ones we flattened\n",
        "# d = example_image.flatten().size()[0]\n",
        "# print(c * w * h == d)\n",
        "\n",
        "# # View the image\n",
        "# t =  torchvision.transforms.ToPILImage()\n",
        "# plt.imshow(t(example_image))\n",
        "\n",
        "# # These are what the class labels in CIFAR-10 represent. For more information,\n",
        "# # visit https://www.cs.toronto.edu/~kriz/cifar.html\n",
        "# classes = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\",\n",
        "#            \"horse\", \"ship\", \"truck\"]\n",
        "# print(f\"This image is labeled as class {classes[example_label]}\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JdBbLQdC_mA_"
      },
      "source": [
        "In this problem, we will attempt to predict what class an image is labeled as."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "J3HQyGMn_42D"
      },
      "source": [
        "First, let's create our model. For a linear model we could flatten the data before passing it into the model, but that is not be the case for the convolutional neural network."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kd49udL8AZ_E"
      },
      "source": [
        "Let's define a method to train this model using SGD as our optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YgcFP1-UBj1Z"
      },
      "outputs": [],
      "source": [
        "# def train(\n",
        "#     model: nn.Module, optimizer: SGD,\n",
        "#     train_loader: DataLoader, val_loader: DataLoader,\n",
        "#     epochs: int = 20\n",
        "# )-> Tuple[List[float], List[float], List[float], List[float]]:\n",
        "#     \"\"\"\n",
        "#     Trains a model for the specified number of epochs using the loaders.\n",
        "\n",
        "#     Returns: \n",
        "#     Lists of training loss, training accuracy, validation loss, validation accuracy for each epoch.\n",
        "#     \"\"\"\n",
        "\n",
        "#     loss = nn.CrossEntropyLoss()\n",
        "#     train_losses = []\n",
        "#     train_accuracies = []\n",
        "#     val_losses = []\n",
        "#     val_accuracies = []\n",
        "#     for e in tqdm(range(epochs)):\n",
        "#         model.train()\n",
        "#         train_loss = 0.0\n",
        "#         train_acc = 0.0\n",
        "\n",
        "#         # Main training loop; iterate over train_loader. The loop\n",
        "#         # terminates when the train loader finishes iterating, which is one epoch.\n",
        "#         for (x_batch, labels) in train_loader:\n",
        "#             x_batch, labels = x_batch.to(DEVICE), labels.to(DEVICE)\n",
        "#             optimizer.zero_grad()\n",
        "#             labels_pred = model(x_batch)\n",
        "#             batch_loss = loss(labels_pred, labels)\n",
        "#             train_loss = train_loss + batch_loss.item()\n",
        "\n",
        "#             labels_pred_max = torch.argmax(labels_pred, 1)\n",
        "#             batch_acc = torch.sum(labels_pred_max == labels)\n",
        "#             train_acc = train_acc + batch_acc.item()\n",
        "\n",
        "#             batch_loss.backward()\n",
        "#             optimizer.step()\n",
        "#         train_losses.append(train_loss / len(train_loader))\n",
        "#         train_accuracies.append(train_acc / (batch_size * len(train_loader)))\n",
        "\n",
        "#         # Validation loop; use .no_grad() context manager to save memory.\n",
        "#         model.eval()\n",
        "#         val_loss = 0.0\n",
        "#         val_acc = 0.0\n",
        "\n",
        "#         with torch.no_grad():\n",
        "#             for (v_batch, labels) in val_loader:\n",
        "#                 v_batch, labels = v_batch.to(DEVICE), labels.to(DEVICE)\n",
        "#                 labels_pred = model(v_batch)\n",
        "#                 v_batch_loss = loss(labels_pred, labels)\n",
        "#                 val_loss = val_loss + v_batch_loss.item()\n",
        "\n",
        "#                 v_pred_max = torch.argmax(labels_pred, 1)\n",
        "#                 batch_acc = torch.sum(v_pred_max == labels)\n",
        "#                 val_acc = val_acc + batch_acc.item()\n",
        "#             val_losses.append(val_loss / len(val_loader))\n",
        "#             val_accuracies.append(val_acc / (batch_size * len(val_loader)))\n",
        "\n",
        "#     return train_losses, train_accuracies, val_losses, val_accuracies\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZUqV2ZrEf-u"
      },
      "source": [
        "For this problem, we will be using SGD. The two hyperparameters for our linear model trained with SGD are the learning rate and momentum. Only learning rate will be searched for in this example.\n",
        "\n",
        "Note: We ask you to plot the accuracies for the best 5 models for each structure, so you will need to return multiple sets of hyperparameters for the homework, or, if you do random search, run your hyperparameter search multiple times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2p-oDqbyhUvG"
      },
      "outputs": [],
      "source": [
        "# def parameter_search(\n",
        "#     train_loader: DataLoader, \n",
        "#     val_loader: DataLoader, \n",
        "#     model_fn:Callable[[], nn.Module]\n",
        "# ) -> float:\n",
        "#     \"\"\"\n",
        "#     Parameter search for our linear model using SGD.\n",
        "\n",
        "#     Args:\n",
        "#     train_loader: the train dataloader.\n",
        "#     val_loader: the validation dataloader.\n",
        "#     model_fn: a function that, when called, returns a torch.nn.Module.\n",
        "\n",
        "#     Returns:\n",
        "#     The learning rate with the least validation loss.\n",
        "#     NOTE: you may need to modify this function to search over and return\n",
        "#      other parameters beyond learning rate.\n",
        "#     \"\"\"\n",
        "#     num_iter = 10  # This will likely not be enough for the rest of the problem.\n",
        "#     best_loss = torch.tensor(np.inf)\n",
        "#     best_lr = 0.0\n",
        "\n",
        "#     lrs = torch.linspace(10 ** (-6), 10 ** (-1), num_iter)\n",
        "\n",
        "#     for lr in lrs:\n",
        "#         print(f\"trying learning rate {lr}\")\n",
        "#         model = model_fn()\n",
        "#         optim = SGD(model.parameters(), lr)\n",
        "        \n",
        "#         train_loss, train_acc, val_loss, val_acc = train(\n",
        "#             model,\n",
        "#             optim,\n",
        "#             train_loader,\n",
        "#             val_loader,\n",
        "#             epochs=20\n",
        "#             )\n",
        "\n",
        "#         if min(val_loss) < best_loss:\n",
        "#             best_loss = min(val_loss)\n",
        "#             best_lr = lr\n",
        "        \n",
        "#     return best_lr"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZULxD9sGHm1D"
      },
      "source": [
        "Now that we have everything, we can train and evaluate our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDzs8UtJSqsT"
      },
      "outputs": [],
      "source": [
        "# model = linear_model()\n",
        "# optimizer = SGD(model.parameters(), best_lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "d8e645bd20de40238f70d1b5332af937"
          ]
        },
        "id": "_tk07LfkIPgS",
        "outputId": "a71200be-d711-43ae-9d4a-47387cf8b21a"
      },
      "outputs": [],
      "source": [
        "# # We are only using 20 epochs for this example. You may have to use more.\n",
        "# train_loss, train_accuracy, val_loss, val_accuracy = train(\n",
        "#     model, optimizer, train_loader, val_loader, 20\n",
        "# )"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MSi0cEADDJcf"
      },
      "source": [
        "Plot the training and validation accuracy for each epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "-i_KVlY3WthL",
        "outputId": "091bfafc-4961-40a9-94ee-896cca4b97c5"
      },
      "outputs": [],
      "source": [
        "# epochs = range(1, 21)\n",
        "# plt.plot(epochs, train_accuracy, label=\"Train Accuracy\")\n",
        "# plt.plot(epochs, val_accuracy, label=\"Validation Accuracy\")\n",
        "# plt.xlabel(\"Epoch\")\n",
        "# plt.ylabel(\"Accuracy\")\n",
        "# plt.legend()\n",
        "# plt.title(\"Logistic Regression Accuracy for CIFAR-10 vs Epoch\")\n",
        "# plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zu8AaNRl4FDi"
      },
      "source": [
        "The last thing we have to do is evaluate our model on the testing data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KzxYGlYH4MwC"
      },
      "outputs": [],
      "source": [
        "# def evaluate(\n",
        "#     model: nn.Module, loader: DataLoader\n",
        "# ) -> Tuple[float, float]:\n",
        "#     \"\"\"Computes test loss and accuracy of model on loader.\"\"\"\n",
        "#     loss = nn.CrossEntropyLoss()\n",
        "#     model.eval()\n",
        "#     test_loss = 0.0\n",
        "#     test_acc = 0.0\n",
        "#     with torch.no_grad():\n",
        "#         for (batch, labels) in loader:\n",
        "#             batch, labels = batch.to(DEVICE), labels.to(DEVICE)\n",
        "#             y_batch_pred = model(batch)\n",
        "#             batch_loss = loss(y_batch_pred, labels)\n",
        "#             test_loss = test_loss + batch_loss.item()\n",
        "\n",
        "#             pred_max = torch.argmax(y_batch_pred, 1)\n",
        "#             batch_acc = torch.sum(pred_max == labels)\n",
        "#             test_acc = test_acc + batch_acc.item()\n",
        "#         test_loss = test_loss / len(loader)\n",
        "#         test_acc = test_acc / (batch_size * len(loader))\n",
        "#         return test_loss, test_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekfbYD_34XFB",
        "outputId": "97ef26db-193f-40ba-e9d4-3d005e1ed61a"
      },
      "outputs": [],
      "source": [
        "# test_loss, test_acc = evaluate(model, test_loader)\n",
        "# print(f\"Test Accuracy: {test_acc}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "OK-KuYt41uFA"
      },
      "source": [
        "The rest is yours to code. You can structure the code any way you would like.\n",
        "\n",
        "We do advise making using code cells and functions (train, search, predict etc.) for each subproblem, since they will make your code easier to debug. \n",
        "\n",
        "Also note that several of the functions above can be reused for the various different models you will implement for this problem; i.e., you won't need to write a new `evaluate()`."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5JAkBFTvrTcj"
      },
      "source": [
        "## CIFAR-10 neural networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "McPwOn8xrR2-"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "\n",
        "subset = list(range(0, 10000))\n",
        "train_subset = torch.utils.data.Subset(train_dataset, indices=subset)\n",
        "\n",
        "train_data, val_data = random_split(train_subset, [int(0.9 * len(train_subset)), int( 0.1 * len(train_subset))])\n",
        "\n",
        "# Create separate dataloaders for the train, test, and validation set\n",
        "train_loader = DataLoader(\n",
        "    train_data,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_data,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1SE3xwvla6hP"
      },
      "source": [
        "### Fully-connected output, 1 fully-connected hidden layer:\n",
        "\n",
        "$x^{out} = W_2 \\mathrm{relu} (W_1 (x^{in}) + b_1 ) + b_2$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gRW9EyTWFRO"
      },
      "outputs": [],
      "source": [
        "def fully_connected_neural_network(dim_in, m, dim_out) -> nn.Module:\n",
        "    \"\"\"Fully-connected output, 1 fully-connected hidden layer.\"\"\"\n",
        "    model =  nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(dim_in, m),  # [in, out]\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(m, dim_out),\n",
        "            # nn.Linear(m, dim_in),  # [in, out]\n",
        "            # nn.ReLU(),\n",
        "            # nn.Linear(dim_out, m),\n",
        "         )\n",
        "    return model.to(DEVICE)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Parameter search over the fully-connected output, 1 fully-connected hidden layer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(\n",
        "    model: nn.Module, optimizer: SGD,\n",
        "    train_loader: DataLoader, val_loader: DataLoader,\n",
        "    epochs: int = 20\n",
        ")-> Tuple[List[float], List[float], List[float], List[float]]:\n",
        "    \"\"\"\n",
        "    Trains a model for the specified number of epochs using the loaders.\n",
        "\n",
        "    Returns: \n",
        "    Lists of training loss, training accuracy, validation loss, validation accuracy for each epoch.\n",
        "    \"\"\"\n",
        "\n",
        "    loss = nn.CrossEntropyLoss()\n",
        "    train_losses = []\n",
        "    train_accuracies = []\n",
        "    val_losses = []\n",
        "    val_accuracies = []\n",
        "    for e in tqdm(range(epochs)):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_acc = 0.0\n",
        "\n",
        "        # Main training loop; iterate over train_loader. The loop\n",
        "        # terminates when the train loader finishes iterating, which is one epoch.\n",
        "        for (x_batch, labels) in train_loader:\n",
        "            # print(x_batch.shape)\n",
        "            # for p in model.parameters():\n",
        "            #     print(p)\n",
        "\n",
        "            x_batch, labels = x_batch.to(DEVICE), labels.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            labels_pred = model(x_batch)\n",
        "            batch_loss = loss(labels_pred, labels)\n",
        "            train_loss = train_loss + batch_loss.item()\n",
        "\n",
        "            labels_pred_max = torch.argmax(labels_pred, 1)\n",
        "            batch_acc = torch.sum(labels_pred_max == labels)\n",
        "            train_acc = train_acc + batch_acc.item()\n",
        "\n",
        "            batch_loss.backward()\n",
        "            optimizer.step()\n",
        "        train_losses.append(train_loss / len(train_loader))\n",
        "        train_accuracies.append(train_acc / (batch_size * len(train_loader)))\n",
        "\n",
        "        # Validation loop; use .no_grad() context manager to save memory.\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_acc = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for (v_batch, labels) in val_loader:\n",
        "                v_batch, labels = v_batch.to(DEVICE), labels.to(DEVICE)\n",
        "                labels_pred = model(v_batch)\n",
        "                v_batch_loss = loss(labels_pred, labels)\n",
        "                val_loss = val_loss + v_batch_loss.item()\n",
        "\n",
        "                v_pred_max = torch.argmax(labels_pred, 1)\n",
        "                batch_acc = torch.sum(v_pred_max == labels)\n",
        "                val_acc = val_acc + batch_acc.item()\n",
        "            val_losses.append(val_loss / len(val_loader))\n",
        "            val_accuracies.append(val_acc / (batch_size * len(val_loader)))\n",
        "\n",
        "    return train_losses, train_accuracies, val_losses, val_accuracies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "momentum_factors = torch.linspace(0.05, 1.25, 13)\n",
        "momentum_factors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lu_UJcyMHpin"
      },
      "outputs": [],
      "source": [
        "def parameter_search_fully_connected_nn(\n",
        "    train_loader: DataLoader,\n",
        "    val_loader: DataLoader,\n",
        "    model_fn:Callable[[], nn.Module]\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    #TODO: Parameter search for our linear model using SGD.\n",
        "\n",
        "    Args:\n",
        "    train_loader: the train dataloader.\n",
        "    val_loader: the validation dataloader.\n",
        "    model_fn: a function that, when called, returns a torch.nn.Module.\n",
        "\n",
        "    Returns:\n",
        "    TODO: The learning rate with the least validation loss.\n",
        "    \"\"\"\n",
        "    dim_in = 3072\n",
        "    dim_out = 10\n",
        "    learning_rates = torch.logspace(-6, 0, 15)\n",
        "    # hidden_layer_sizes = torch.logspace(2, 9, steps=30, base=2)\n",
        "    # hidden_layer_sizes = torch.logspace(2, 8, steps=7, base=2)\n",
        "    hidden_layer_sizes = torch.linspace(100, 600, 11) #  M\n",
        "    momentum_factors = torch.linspace(0.05, 1.25, 13)\n",
        "    n_epochs = 10 #TODO: increase\n",
        "\n",
        "    num_searches = 20  #TODO: This will likely not be enough for the rest of the problem.\n",
        "    best_loss = torch.tensor(np.inf)\n",
        "\n",
        "\n",
        "    # best_results = {\n",
        "    #     'learning_rate': 0.0,\n",
        "    #     'momentum_factor': 0.0,\n",
        "    #     'hidden_layer_size': 0.0,\n",
        "    #     'train_loss': np.inf,\n",
        "    #     'train_acc': np.inf,\n",
        "    #     'val_loss': np.inf,\n",
        "    #     'val_acc': np.inf,\n",
        "    # }\n",
        "\n",
        "    results = {\n",
        "        'learning_rate': [],\n",
        "        'momentum_factor': [],\n",
        "        'hidden_layer_size': [],\n",
        "        'train_loss': [],\n",
        "        'train_acc': [],\n",
        "        'val_loss': [],\n",
        "        'val_acc': [],\n",
        "    }\n",
        "\n",
        "    for i in range(num_searches):\n",
        "        learning_rate_sampler = torch.randint(low=0, high=len(learning_rates), size=(1,))\n",
        "        hidden_layer_sampler = torch.randint(low=0, high=len(hidden_layer_sizes), size=(1,))\n",
        "        momentum_factor_sampler = torch.randint(low=0, high=len(momentum_factors), size=(1,))\n",
        "\n",
        "        lr = learning_rates[learning_rate_sampler].item()\n",
        "        momentum = momentum_factors[momentum_factor_sampler].item()\n",
        "        m = int(hidden_layer_sizes[hidden_layer_sampler].item())\n",
        "\n",
        "        print(f\"lr: {lr}; momentum: {momentum}; m: {m}\")\n",
        "\n",
        "        model = model_fn(dim_in, m, dim_out)\n",
        "        optim = SGD(model.parameters(), lr=lr, momentum=momentum)\n",
        "\n",
        "        train_loss, train_acc, val_loss, val_acc = train(\n",
        "            model,\n",
        "            optim,\n",
        "            train_loader,\n",
        "            val_loader,\n",
        "            epochs=n_epochs\n",
        "        )\n",
        "\n",
        "        #TODO: append all to a list?\n",
        "        results['learning_rate'].append(lr)\n",
        "        results['momentum_factor'].append(momentum)\n",
        "        results['hidden_layer_size'].append(m)\n",
        "        results['train_loss'].append(train_loss)\n",
        "        results['train_acc'].append(train_acc)\n",
        "        results['val_loss'].append(val_loss)\n",
        "        results['val_acc'].append(val_acc)\n",
        "\n",
        "        # if min(val_loss) < best_loss:\n",
        "        #     best_results['learning_rate'] = lr\n",
        "        #     best_results['momentum_factor'] = momentum\n",
        "        #     best_results['hidden_layer_size'] = m\n",
        "        #     best_results['train_loss'] = min(train_loss)\n",
        "        #     best_results['train_acc'] = min(train_acc)\n",
        "        #     best_results['val_loss'] = min(val_loss)\n",
        "        #     best_results['val_acc'] = min(val_acc)\n",
        "        #     print(best_results)\n",
        "\n",
        "    return results #best_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHHtWAQHJPcN"
      },
      "outputs": [],
      "source": [
        "parameters = parameter_search_fully_connected_nn(train_loader,\n",
        "                                                 val_loader,\n",
        "                                                 fully_connected_neural_network)\n",
        "\n",
        "for key, item in parameters.items():\n",
        "    parameters[key] = np.array(item)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# val_acc_sorted = np.take_along_axis(parameters['val_acc'], sort_by_val_acc, axis=1)\n",
        "# train_acc_sorted = np.take_along_axis(parameters['train_acc'], sort_by_val_acc, axis=1)\n",
        "# train_acc_sorted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_search_index = np.argmax(np.max(parameters['val_acc'], axis=1))\n",
        "# best_parameters = {p: parameters[p][best_search_index] for p in parameters.keys()}\n",
        "# print(best_search_index)\n",
        "best_3_search_indices = np.argpartition(np.max(parameters['val_acc'][:,-5:], axis=1), -3)[-3:]\n",
        "best_3_search_indices = best_3_search_indices[np.argsort(np.max(parameters['val_acc'], axis=1)[best_3_search_indices])]\n",
        "best_3_search_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "epochs = range(0, len(parameters['val_acc'][0]))\n",
        "num_searches = len(parameters['learning_rate'])\n",
        "\n",
        "# sort_by_val_acc = np.argsort(parameters['val_acc'], axis=1)\n",
        "# val_acc_sorted = np.take_along_axis(parameters['val_acc'], sort_by_val_acc, axis=1)\n",
        "# train_acc_sorted = np.take_along_axis(parameters['train_acc'], sort_by_val_acc, axis=1)\n",
        "\n",
        "# val_acc_sorted = parameters['val_acc']\n",
        "# train_acc_sorted = parameters['train_acc']\n",
        "# val_acc_sorted = parameters['val_acc'][range(0,10),sort_by_val_acc]\n",
        "# train_acc_sorted = np.sort(parameters['train_acc'], axis=1)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8,8))\n",
        "# for val_acc, train_acc in zip(val_acc_sorted, train_acc_sorted):\n",
        "# for i in range(num_searches):\n",
        "for i in best_3_search_indices:\n",
        "    ax.plot(epochs, parameters['val_acc'][i], label='validation')\n",
        "    # ax.plot(epochs, train_acc, label='train')\n",
        "    ax.set_ylabel('validation accuracy')\n",
        "    ax.set_xlabel('epoch')\n",
        "    # ax.legend(frameon=False)\n",
        "    ax.set_ylim([0, 0.6])\n",
        "    line_label = (f\"lr={np.round(parameters['learning_rate'][i], 8)}; \"\n",
        "                  f\"mom={np.round(parameters['momentum_factor'][i], 5)}; \"\n",
        "                  f\"m={parameters['hidden_layer_size'][i]}\")\n",
        "    ax.annotate(line_label, (epochs[-1], parameters['val_acc'][i][-1]))\n",
        "\n",
        "# fig, ax = plt.subplots(figsize=(6,3))\n",
        "# ax.plot(epochs, val_loss, label='validation')\n",
        "# ax.plot(epochs, train_loss, label='train')\n",
        "# ax.set_ylabel('loss')\n",
        "# ax.set_xlabel('epoch')\n",
        "# ax.legend(frameon=False)\n",
        "# ax.set_ylim([1, 3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "search_index = best_3_search_indices[0]\n",
        "\n",
        "print(search_index)\n",
        "\n",
        "lr = parameters['learning_rate'][search_index]\n",
        "momentum = parameters['momentum_factor'][search_index]\n",
        "m = parameters['hidden_layer_size'][search_index]\n",
        "\n",
        "print(f\"lr={np.round(lr, 5)}; \"\n",
        "      f\"mom={np.round(momentum, 5)}; \"\n",
        "      f\"m={m}\")\n",
        "\n",
        "print(np.max(parameters['val_acc'], axis=1)[search_index])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Retrain the best-performing model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dim_in = 3072\n",
        "dim_out = 10\n",
        "\n",
        "lr = parameters['learning_rate'][search_index]\n",
        "momentum = parameters['momentum_factor'][search_index]\n",
        "m = parameters['hidden_layer_size'][search_index]\n",
        "\n",
        "# learning_rate = 0.001\n",
        "# momentum = 0.6\n",
        "# m = 16\n",
        "\n",
        "\n",
        "print(f\"lr={np.round(lr, 5)}; \"\n",
        "      f\"mom={np.round(momentum, 5)}; \"\n",
        "      f\"m={m}\")\n",
        "\n",
        "n_epochs = 100 #TODO: increase\n",
        "\n",
        "model = fully_connected_neural_network(dim_in, m, dim_out)\n",
        "optim = SGD(model.parameters(), lr=lr, momentum=momentum)\n",
        "\n",
        "train_loss, train_acc, val_loss, val_acc = train(\n",
        "    model,\n",
        "    optim,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    epochs=n_epochs\n",
        ")\n",
        "\n",
        "print(f'train_loss: {train_loss}')\n",
        "print(f'train_acc: {train_acc}')\n",
        "print(f'val_loss: {val_loss}')\n",
        "print(f'val_acc: {val_acc}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "epochs = range(0, n_epochs)\n",
        "fig, ax = plt.subplots(figsize=(6,3))\n",
        "ax.plot(epochs, val_acc, label='validation')\n",
        "ax.plot(epochs, train_acc, label='train')\n",
        "ax.set_ylabel('accuracy')\n",
        "ax.set_xlabel('epoch')\n",
        "ax.legend(frameon=False)\n",
        "ax.set_ylim([0, 0.6])\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6,3))\n",
        "ax.plot(epochs, val_loss, label='validation')\n",
        "ax.plot(epochs, train_loss, label='train')\n",
        "ax.set_ylabel('loss')\n",
        "ax.set_xlabel('epoch')\n",
        "ax.legend(frameon=False)\n",
        "ax.set_ylim([1, 3])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Convolutional layer with max-pool and fully-connected output:\n",
        "\n",
        "$x^{out} = W_2 (\\mathrm{MaxPool} (\\mathrm{relu} ( \\mathrm{Conv2d} (x^{in}, W_1) + b_1 ))) + b_2$\n",
        "\n",
        "Where,\n",
        "\n",
        "$\\mathrm{Conv2d} (x^{in}, W_1) \\in \\R^{(33-k) \\times (33-k) \\times M}$\n",
        "\n",
        "$\\mathrm{MaxPool} (\\mathrm{relu} ( \\mathrm{Conv2d} (x^{in}, W_1) + b_1 )) \\in \\R^{(\\frac{33-k}{N}) \\times (\\frac{33-k}{N}) \\times M}$\n",
        "\n",
        "$W_2 \\in \\R^{10 \\times M(\\frac{33-k}{N})^2};\\; b_2 \\in \\R^{10}$\n",
        "\n",
        "such that $M$, $k$, $N$ are model-specific hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dim_in = 3\n",
        "m = 150\n",
        "k = 5\n",
        "n = 14\n",
        "\n",
        "int(m * ((33 - k)/n)**2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def convolutional_neural_network(dim_in, m, k, n, dim_out) -> nn.Module:\n",
        "    \"\"\"Fully-connected output, 1 fully-connected hidden layer.\"\"\"\n",
        "    fc_input_size = int(m * ((33 - k)/n)**2)\n",
        "\n",
        "    print(fc_input_size)\n",
        "    model =  nn.Sequential(\n",
        "            nn.Conv2d(dim_in, m, k),  # (in, # filters, kernel size)\n",
        "            # nn.Conv2d(dim_in, m, kernel_size=(k, k, 3)),  # (in, # filters, kernel size)\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=(n, n)),\n",
        "            nn.Flatten(),\n",
        "            # nn.Linear(dim_in, dim_out),\n",
        "            nn.Linear(fc_input_size, dim_out),\n",
        "         )\n",
        "    return model.to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "p = np.array([2, 3, 4, 5])\n",
        "k = np.array([4, 5, 6, 8])\n",
        "n = np.divide.outer(33-k, p)\n",
        "n\n",
        "k_p_combs = [(5, 2), (5, 4), (6, 3), (8, 5)]\n",
        "k_n_combs = [(5, 14), (5, 7), (6, 9), (8, 5)]\n",
        "for k, p in k_p_combs:\n",
        "    print(f\"{k},{p}\")\n",
        "    n = np.divide.outer(33-k, p)\n",
        "    print(f\"n={n}\")\n",
        "\n",
        "for k, n in k_n_combs:\n",
        "    print(f\"{k},{n}\")\n",
        "    p = np.divide(33-k, n)\n",
        "    print(f\"p={p}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Parameter search over the convolutional layer with max-pool and fully-connected output:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parameter_search_convolutional_nn(\n",
        "    train_loader: DataLoader,\n",
        "    val_loader: DataLoader,\n",
        "    model_fn:Callable[[], nn.Module]\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Parameter search over the the convolutional layer with max-pool and fully-\n",
        "    connected output.\n",
        "\n",
        "    Args:\n",
        "    train_loader: the train dataloader.\n",
        "    val_loader: the validation dataloader.\n",
        "    model_fn: a function that, when called, returns a torch.nn.Module.\n",
        "\n",
        "    Returns:\n",
        "    Hyperparameters and train/validation losses and accuracy over the epochs of\n",
        "    a random search.\n",
        "    \"\"\"\n",
        "    dim_in = 3\n",
        "    dim_out = 10\n",
        "    learning_rates = torch.logspace(-6, 0, 15)\n",
        "    momentum_factors = torch.linspace(0.05, 1.5, 20)\n",
        "\n",
        "    conv2d_filters = torch.tensor([10, 20, 50, 100, 120, 150, 200])  # m_typ = 100; = torch.logspace(2, 9, steps=30, base=2)\n",
        "    k_n_combs = [(5, 14), (5, 7), (6, 9), (8, 5)]  # k_typ = 5, n_typ = 14; pool_size = np.divide(33-k, n)\n",
        "    n_epochs = 8\n",
        "    num_searches = 3\n",
        "\n",
        "    results = {\n",
        "        'learning_rate': [],\n",
        "        'momentum_factor': [],\n",
        "        'conv2d_size': [],  # k\n",
        "        'conv2d_filters': [],  # M\n",
        "        'maxpool_size': [],  # N\n",
        "        'train_loss': [],\n",
        "        'train_acc': [],\n",
        "        'val_loss': [],\n",
        "        'val_acc': [],\n",
        "    }\n",
        "\n",
        "    for i in range(num_searches):\n",
        "        learning_rate_sampler = torch.randint(low=0, high=len(learning_rates), size=(1,))\n",
        "        momentum_factor_sampler = torch.randint(low=0, high=len(momentum_factors), size=(1,))\n",
        "        conv2d_filters_sampler = torch.randint(low=0, high=len(conv2d_filters), size=(1,))\n",
        "        k_n_combs_sampler = torch.randint(low=0, high=len(k_n_combs), size=(1,))\n",
        "\n",
        "\n",
        "        lr = learning_rates[learning_rate_sampler].item()\n",
        "        momentum = momentum_factors[momentum_factor_sampler].item()\n",
        "        m = int(conv2d_filters[conv2d_filters_sampler].item())\n",
        "        k, n = k_n_combs[k_n_combs_sampler]\n",
        "\n",
        "        print(f\"lr: {lr}; \"\n",
        "              f\"momentum: {momentum}; \"\n",
        "              f\"m: {m}; \"\n",
        "              f\"k: {k}; \"\n",
        "              f\"n: {n}; \")\n",
        "\n",
        "        model = model_fn(dim_in, m, k, n, dim_out)\n",
        "\n",
        "        optim = SGD(model.parameters(), lr=lr, momentum=momentum)\n",
        "\n",
        "        train_loss, train_acc, val_loss, val_acc = train(\n",
        "            model,\n",
        "            optim,\n",
        "            train_loader,\n",
        "            val_loader,\n",
        "            epochs=n_epochs\n",
        "        )\n",
        "\n",
        "        results['learning_rate'].append(lr)\n",
        "        results['momentum_factor'].append(momentum)\n",
        "        results['conv2d_size'].append(k)\n",
        "        results['conv2d_filters'].append(m)\n",
        "        results['maxpool_size'].append(n)\n",
        "        results['train_loss'].append(train_loss)\n",
        "        results['train_acc'].append(train_acc)\n",
        "        results['val_loss'].append(val_loss)\n",
        "        results['val_acc'].append(val_acc)\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "parameters = parameter_search_convolutional_nn(train_loader,\n",
        "                                               val_loader,\n",
        "                                               convolutional_neural_network)\n",
        "\n",
        "for key, item in parameters.items():\n",
        "    parameters[key] = np.array(item)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate(\n",
        "    model: nn.Module, loader: DataLoader\n",
        ") -> Tuple[float, float]:\n",
        "    \"\"\"Computes test loss and accuracy of model on loader.\"\"\"\n",
        "    loss = nn.CrossEntropyLoss()\n",
        "    model.eval()\n",
        "    test_loss = 0.0\n",
        "    test_acc = 0.0\n",
        "    with torch.no_grad():\n",
        "        for (batch, labels) in loader:\n",
        "            batch, labels = batch.to(DEVICE), labels.to(DEVICE)\n",
        "            y_batch_pred = model(batch)\n",
        "            batch_loss = loss(y_batch_pred, labels)\n",
        "            test_loss = test_loss + batch_loss.item()\n",
        "\n",
        "            pred_max = torch.argmax(y_batch_pred, 1)\n",
        "            batch_acc = torch.sum(pred_max == labels)\n",
        "            test_acc = test_acc + batch_acc.item()\n",
        "        test_loss = test_loss / len(loader)\n",
        "        test_acc = test_acc / (batch_size * len(loader))\n",
        "        return test_loss, test_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_search_index = np.argmax(np.max(parameters['val_acc'], axis=1))\n",
        "best_3_search_indices = np.argpartition(np.max(parameters['val_acc'][:,-5:], axis=1), -3)[-3:]\n",
        "best_3_search_indices = best_3_search_indices[np.argsort(np.max(parameters['val_acc'], axis=1)[best_3_search_indices])]\n",
        "best_3_search_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "epochs = range(0, len(parameters['val_acc'][0]))\n",
        "num_searches = len(parameters['learning_rate'])\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8,6))\n",
        "for i in range(num_searches):\n",
        "# for i in best_3_search_indices:\n",
        "    ax.plot(epochs, parameters['val_acc'][i], label='validation')\n",
        "    # ax.plot(epochs, train_acc, label='train')\n",
        "    ax.set_ylabel('validation accuracy')\n",
        "    ax.set_xlabel('epoch')\n",
        "    # ax.legend(frameon=False)\n",
        "    ax.set_ylim([0, 0.6])\n",
        "    line_label = (f\"lr={np.round(parameters['learning_rate'][i], 5)}; \"\n",
        "                  f\"mom={np.round(parameters['momentum_factor'][i], 2)}; \"\n",
        "                  f\"k={np.round(parameters['conv2d_size'][i], 5)}; \"\n",
        "                  f\"M={np.round(parameters['conv2d_filters'][i], 5)}; \"\n",
        "                  f\"N={parameters['maxpool_size'][i]}\")\n",
        "\n",
        "    ax.annotate(line_label, (epochs[-1], parameters['val_acc'][i][-1]))\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8,6))\n",
        "for i in range(num_searches):\n",
        "    ax.plot(epochs, parameters['val_loss'][i], label='validation')\n",
        "    # ax.plot(epochs, parameters['train_loss'][i], label='train')\n",
        "    ax.set_ylabel('loss')\n",
        "    ax.set_xlabel('epoch')\n",
        "    ax.legend(frameon=False)\n",
        "    ax.set_ylim([1, 3])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "cse446",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "vscode": {
      "interpreter": {
        "hash": "bf814fda1cc440ac317e22279a1ec33d1b20faeecc9ea242b28923e33d4f784d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
